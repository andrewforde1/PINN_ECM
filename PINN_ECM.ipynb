{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e3871c",
   "metadata": {
    "id": "d1e3871c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.set_default_device('cuda')\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "my_dtype = torch.float64\n",
    "torch.set_default_dtype(my_dtype)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c7f51b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_nasa_cycle(cycle_num):\n",
    "    data_file = scipy.io.loadmat(file_path, squeeze_me=False)\n",
    "    \n",
    "    data = {}\n",
    "    data['Imeas'] = torch.tensor(data_file[file_name]['cycle'][0][0][0, cycle_num]['data'][0, 0]['Current_measured'][0, :], requires_grad=True).unsqueeze(-1)\n",
    "    data['Vmeas'] = torch.tensor(data_file[file_name]['cycle'][0][0][0, cycle_num]['data'][0, 0]['Voltage_measured'][0, :], requires_grad=True).unsqueeze(-1)\n",
    "    data['T'] = torch.tensor(data_file[file_name]['cycle'][0][0][0, cycle_num]['data'][0, 0]['Temperature_measured'][0, :], requires_grad=True).unsqueeze(-1)\n",
    "    data['time'] = torch.tensor(data_file[file_name]['cycle'][0][0][0, cycle_num]['data'][0, 0]['Time'][0, :], requires_grad=True).unsqueeze(-1)\n",
    "    data['cycle_type'] = data_file[file_name]['cycle'][0][0][0, cycle_num]['type'][0]\n",
    "    \n",
    "    # Capacity is measured on discharge cycles so only available for these\n",
    "    if data['cycle_type'] == 'discharge':\n",
    "        data['capacity'] = data_file[file_name]['cycle'][0][0][0, cycle_num]['data'][0, 0]['Capacity'][0, 0]\n",
    "        data['Q_nominal'] = torch.tensor(data['capacity']*3600.0, requires_grad=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def read_nissan_cycle(filepath, uniform_timestep=True, uniform_dt=1.0):\n",
    "    data = {}\n",
    "    \n",
    "    csv_file = np.loadtxt(filepath, delimiter=',', skiprows=1, usecols=(1,7,8,9,11))\n",
    "    \n",
    "    data['capacity']  = 30.0\n",
    "    data['Q_nominal'] = torch.tensor(data['capacity']*3600.0, requires_grad=True)\n",
    "    \n",
    "    data['SOC_t0'] = csv_file[0,4] / data['capacity']\n",
    "    \n",
    "    if uniform_timestep:\n",
    "        num_steps     = int((csv_file[-1,0] - csv_file[0,0]) / uniform_dt)\n",
    "        data['time']  = np.linspace(csv_file[0,0], csv_file[-1,0], num_steps)\n",
    "        data['Imeas'] = torch.from_numpy(np.interp(data['time'], csv_file[:,0], csv_file[:,2])).unsqueeze(-1).to('cuda')\n",
    "        data['Vmeas'] = torch.from_numpy(np.interp(data['time'], csv_file[:,0], csv_file[:,3])).unsqueeze(-1).to('cuda')\n",
    "        data['time']  = torch.from_numpy(data['time']).unsqueeze(-1).to('cuda')\n",
    "        data['dt']    = torch.diff(data['time'], dim=0, prepend=torch.zeros((1,1))).to('cuda')\n",
    "        \n",
    "        data['time'].requires_grad=True\n",
    "        data['dt'].requires_grad=True\n",
    "        data['Imeas'].requires_grad=True\n",
    "        data['Vmeas'].requires_grad=True\n",
    "\n",
    "        \n",
    "    else:\n",
    "        data['time']  = torch.tensor(csv_file[:,0], requires_grad=True).unsqueeze(-1)\n",
    "        data['dt']    = torch.tensor(csv_file[:,1], requires_grad=True).unsqueeze(-1)\n",
    "        data['Imeas'] = torch.tensor(csv_file[:,2], requires_grad=True).unsqueeze(-1)\n",
    "        data['Vmeas'] = torch.tensor(csv_file[:,3], requires_grad=True).unsqueeze(-1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab70734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "N_file_start = 0\n",
    "N_file_end = 500 #2287  # Number of timesteps to take from file (starting at beginning)\n",
    "n_skips    = 1      # Sample data from file at this interval\n",
    "N = N_file_end - N_file_start\n",
    "\n",
    "file_type = 'nissan'\n",
    "file_name = 'B0005'\n",
    "file_path = [\n",
    "             {\"path\" : \"D:/OneDrive - Queen's University Belfast/Documents/Nissan_Data/cell-discharge-bitrode-1c.csv\",\n",
    "              \"name\" : \"cell-discharge-bitrode-1c\"},\n",
    "             {\"path\" : \"D:/OneDrive - Queen's University Belfast/Documents/Nissan_Data/cell-discharge-bitrode-2c.csv\",\n",
    "              \"name\" : \"cell-discharge-bitrode-2c\"},\n",
    "             {\"path\" : \"D:/OneDrive - Queen's University Belfast/Documents/Nissan_Data/cell-discharge-bitrode-3c.csv\",\n",
    "              \"name\" : \"cell-discharge-bitrode-3c\"},\n",
    "             {\"path\" : \"D:/OneDrive - Queen's University Belfast/Documents/Nissan_Data/cell-low-current-hppc-10c.csv\",\n",
    "              \"name\" : \"cell-low-current-hppc-10c\"},\n",
    "             {\"path\" : \"D:/OneDrive - Queen's University Belfast/Documents/Nissan_Data/cell-low-current-hppc-25c-2.csv\",\n",
    "              \"name\" : \"cell-low-current-hppc-25c-2\"},\n",
    "             {\"path\" : \"D:/OneDrive - Queen's University Belfast/Documents/Nissan_Data/cell-low-current-hppc-40c-1.csv\",\n",
    "              \"name\" : \"cell-low-current-hppc-40c-1\"},\n",
    "            ]\n",
    "\n",
    "# Load data file and cut down to desired length\n",
    "if file_type == 'nasa':\n",
    "    data_file = read_nasa_cycle(1)\n",
    "    print(data_file['cycle_type'])\n",
    "    for var in data_file:\n",
    "        if var == 'Imeas' or var == 'Vmeas' or var == 'T' or var == 'time':\n",
    "            data_file[var] = data_file[var][N_file_start:N_file_end]\n",
    "            data_file[var] = torch.tensor(data_file[var][::n_skips], requires_grad=True)\n",
    "\n",
    "    dt = torch.tensor(torch.diff(data_file['time'][0,:,0].cpu().detach()), requires_grad=True, dtype=my_dtype)\n",
    "    dt = torch.concat((dt, dt[-1:,:]), dim=0)\n",
    "    \n",
    "elif file_type == 'nissan':\n",
    "    \n",
    "    #Loop through cycle files\n",
    "    file_num = 0\n",
    "    data_file = {}\n",
    "    for file in file_path:\n",
    "        \n",
    "        # Read this cycle file\n",
    "        data_file[file_num] = read_nissan_cycle(file['path'], uniform_timestep=False)\n",
    "\n",
    "        for var in data_file[file_num]:\n",
    "            if var == 'Imeas' or var == 'Vmeas' or var == 'T' or var == 'time':\n",
    "                data_file[file_num][var] = data_file[file_num][var][N_file_start:N_file_end]\n",
    "                data_file[file_num][var] = torch.tensor(data_file[file_num][var][::n_skips], requires_grad=True)\n",
    "\n",
    "        data_file[file_num]['dt'] = torch.diff(data_file[file_num]['time'], dim=0, prepend=torch.zeros((1,1)))\n",
    "                \n",
    "        if file['name'] == \"cell-discharge-bitrode-2c\":\n",
    "            data_file[file_num]['SOC_t0'] = torch.tensor(1.0, requires_grad=True)\n",
    "            \n",
    "        elif file['name'] == \"cell-discharge-bitrode-3c\":\n",
    "            data_file[file_num]['SOC_t0'] = torch.tensor(1.0, requires_grad=True)\n",
    "            \n",
    "        else:\n",
    "            data_file[file_num]['SOC_t0'] = torch.tensor(0.0, requires_grad=True)\n",
    "            \n",
    "        file_num += 1\n",
    "\n",
    "            \n",
    "with torch.no_grad():\n",
    "    plot_n = 0\n",
    "    \n",
    "    dSOC = torch.zeros((len(data_file), N, 1))\n",
    "    SOC  = torch.zeros_like(dSOC)\n",
    "            \n",
    "    if file_type == 'nasa':\n",
    "        if data_file[plot_n]['cycle_type'] == 'discharge':\n",
    "            SOC[:,0,0] = 1.0  \n",
    "            \n",
    "    elif file_type == 'nissan':\n",
    "        for ii in range(len(data_file)):\n",
    "            dSOC[ii] = (data_file[ii]['Imeas'] / data_file[ii]['Q_nominal']) * data_file[ii]['dt']\n",
    "            SOC[ii,0,0] = data_file[ii]['SOC_t0']\n",
    "            \n",
    "    for i in tqdm(range(1, SOC.size(1))):\n",
    "        SOC[:,i,0] = SOC[:,i-1,0] + dSOC[:,i-1,0]\n",
    "    \n",
    "    for ii in range(len(data_file)):\n",
    "        plt.plot(SOC[ii,:,0].cpu())\n",
    "    plt.ylabel('SOC')\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27888764",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECM_standard(nn.Module):\n",
    "    def __init__(self, H1, H2, H3, n_lstm):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.H1 = H1\n",
    "        self.H2 = H2\n",
    "        self.H3 = H3\n",
    "        self.n_lstm = n_lstm\n",
    "        \n",
    "        self.act = torch.tanh\n",
    "        \n",
    "        # Training records\n",
    "        self.epochs = 0\n",
    "        self.data_loss = []\n",
    "        self.theory_loss = []\n",
    "\n",
    "        self.inp = nn.Linear(2, self.H2)\n",
    "\n",
    "        self.encoder  = nn.Linear(self.H1, self.H2)\n",
    "        self.encoder2 = nn.Linear(self.H2, self.H2)\n",
    "        \n",
    "        self.rnn     = nn.LSTM(input_size=self.H2,\n",
    "                               hidden_size=self.H3,\n",
    "                               num_layers=self.n_lstm,\n",
    "                               batch_first=True,\n",
    "                               bidirectional=False)\n",
    "        \n",
    "        self.decoder = nn.Linear(self.H3, self.H2)\n",
    "        self.decoder2 = nn.Linear(self.H2, 1)\n",
    "        \n",
    "        self.bn_inp = nn.BatchNorm1d(H2)\n",
    "        self.bn_enc1 = nn.BatchNorm1d(H2)\n",
    "        self.bn_enc2 = nn.BatchNorm1d(H2)\n",
    "        self.bn_lstm = nn.BatchNorm1d(H3)\n",
    "        self.bn_dec1 = nn.BatchNorm1d(H2)\n",
    "        self.bn_dec2 = nn.BatchNorm1d(2)\n",
    "    \n",
    "    def forward(self, x_in, hidden_states):\n",
    "        x = self.act(self.inp(x_in))\n",
    "        x_lstm, (h1, c1) = self.rnn(x, hidden_states)\n",
    "        x = self.act(self.decoder(x_lstm))\n",
    "        x = self.decoder2(x)\n",
    "        return x, (h1, c1)\n",
    "    \n",
    "    def closure(self, y_true, x, hidden_states1, I1_t0, I2_t0):\n",
    "        h1, c1 = hidden_states1\n",
    "\n",
    "        # predict\n",
    "        y_pred, (h1, c1) = model(x, (h1, c1))\n",
    "\n",
    "        # calculate loss\n",
    "        loss = nn.MSELoss()(y_pred, y_true)\n",
    "        \n",
    "        self.data_loss.append(loss.item())\n",
    "        \n",
    "        _ = torch.empty(1,) # empty tensor to make this compatible with the PINN closure\n",
    "\n",
    "        return loss, (h1, c1), _, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1b6c20",
   "metadata": {
    "id": "4e1b6c20"
   },
   "outputs": [],
   "source": [
    "class ECM_PINN(nn.Module):\n",
    "    def __init__(self, H1, H2, H3, n_lstm):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.L_data = torch.tensor(1.0, requires_grad=True)\n",
    "        self.L_theory = torch.tensor(1.0, requires_grad=True)\n",
    "        self.L_ECM = torch.tensor(1.0e2, requires_grad=True)\n",
    "        \n",
    "        self.H1 = H1\n",
    "        self.H2 = H2\n",
    "        self.H3 = H3\n",
    "        self.n_lstm = n_lstm\n",
    "        \n",
    "        self.act = torch.tanh\n",
    "\n",
    "        self.inp = nn.Linear(3, self.H2)\n",
    "\n",
    "        self.encoder = nn.Linear(self.H1, self.H2)\n",
    "        self.encoder2 = nn.Linear(self.H2, self.H2)\n",
    "        \n",
    "        self.rnn     = nn.LSTM(input_size=self.H2,\n",
    "                               hidden_size=self.H3,\n",
    "                               num_layers=self.n_lstm,\n",
    "                               batch_first=True,\n",
    "                               bidirectional=False)\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.Linear(self.H3, self.H2)\n",
    "        self.decoder2 = nn.Linear(self.H2, 1)\n",
    "        self.out_bias = torch.tensor(2.8, requires_grad=True)\n",
    "            \n",
    "        self.soc_decoder = nn.Linear(self.H2, 1)\n",
    "        self.v_decoder   = nn.Linear(self.H2, 1)\n",
    "\n",
    "        self.bn_inp = nn.BatchNorm1d(H2)\n",
    "        self.bn_enc1 = nn.BatchNorm1d(H2)\n",
    "        self.bn_enc2 = nn.BatchNorm1d(H2)\n",
    "        self.bn_lstm = nn.BatchNorm1d(H3)\n",
    "        self.bn_dec1_V = nn.BatchNorm1d(H2)\n",
    "        self.bn_dec1_soc = nn.BatchNorm1d(H2)\n",
    "        \n",
    "        '''\n",
    "            ECM Parameters\n",
    "        '''        \n",
    "        self.Q_nominal = torch.tensor(25*3600.0, requires_grad=True)\n",
    "        \n",
    "        # LUT values\n",
    "        self.R0_0     = nn.Parameter(torch.tensor(1.0e-5, requires_grad=True))\n",
    "        self.R1_0     = nn.Parameter(torch.tensor(1.0e-5, requires_grad=True))\n",
    "        self.R2_0     = nn.Parameter(torch.tensor(1.0e-5, requires_grad=True))\n",
    "        self.R0_1_SOC = nn.Parameter(torch.tensor(1.0e-5, requires_grad=True))\n",
    "        self.R1_1_SOC = nn.Parameter(torch.tensor(1.0e-5, requires_grad=True))\n",
    "        self.R2_1_SOC = nn.Parameter(torch.tensor(1.0e-5, requires_grad=True))\n",
    "        self.R0_2_SOC = nn.Parameter(torch.tensor(1.0e-5, requires_grad=True))\n",
    "        self.R1_2_SOC = nn.Parameter(torch.tensor(1.0e-5, requires_grad=True))\n",
    "        self.R2_2_SOC = nn.Parameter(torch.tensor(1.0e-5, requires_grad=True))\n",
    "        \n",
    "        self.C1_0     = nn.Parameter(torch.tensor(1.0, requires_grad=True))\n",
    "        self.C2_0     = nn.Parameter(torch.tensor(1.0, requires_grad=True))\n",
    "        self.C1_1_SOC = nn.Parameter(torch.tensor(1.0e-5, requires_grad=True))\n",
    "        self.C2_1_SOC = nn.Parameter(torch.tensor(1.0e-5, requires_grad=True))\n",
    "        self.C1_2_SOC = nn.Parameter(torch.tensor(1.0e-5, requires_grad=True))\n",
    "        self.C2_2_SOC = nn.Parameter(torch.tensor(1.0e-5, requires_grad=True))\n",
    "        \n",
    "        self.Vocv_0      = nn.Parameter(torch.tensor(2.8, requires_grad=True))\n",
    "\n",
    "        N_LUT = 3\n",
    "        self.Vocv_l1 = nn.Linear(1, N_LUT, bias=True)\n",
    "        self.Vocv_l2 = nn.Linear(N_LUT, 1, bias=True)\n",
    "        \n",
    "        self.R0_l1 = nn.Linear(1, N_LUT, bias=True)\n",
    "        self.R0_l2 = nn.Linear(N_LUT, 1, bias=True)\n",
    "        \n",
    "        self.R1_l1 = nn.Linear(1, N_LUT, bias=True)\n",
    "        self.R1_l2 = nn.Linear(N_LUT, 1, bias=True)\n",
    "        \n",
    "        self.R2_l1 = nn.Linear(1, N_LUT, bias=True)\n",
    "        self.R2_l2 = nn.Linear(N_LUT, 1, bias=True)\n",
    "        \n",
    "        self.C1_l1 = nn.Linear(1, N_LUT, bias=True)\n",
    "        self.C1_l2 = nn.Linear(N_LUT, 1, bias=True)\n",
    "        \n",
    "        self.C2_l1 = nn.Linear(1, N_LUT, bias=True)\n",
    "        self.C2_l2 = nn.Linear(N_LUT, 1, bias=True)\n",
    "\n",
    "        \n",
    "        # Training records\n",
    "        self.epochs = 0\n",
    "        self.theory_loss = []\n",
    "        self.data_loss = []\n",
    "        self.ecm_loss = []\n",
    "        self.theory_loss_batch = []\n",
    "        self.data_loss_batch = []\n",
    "        self.ecm_loss_batch = []\n",
    "        \n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            #module.weight.data.normal_(mean=0.0, std=0.1)\n",
    "            y = 1.0 / torch.sqrt(torch.tensor(torch.numel(module.weight.data), dtype=my_dtype))\n",
    "            module.weight.data.normal_(0.0, y)\n",
    "            \n",
    "            if module.bias is not None:\n",
    "                module.bias.data.fill_(0.01)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "        FORWARD PASS\n",
    "    '''\n",
    "    \n",
    "    def forward(self, x_in, hidden_states):\n",
    "        x = self.act(self.inp(x_in))\n",
    "        x_lstm, (h1, c1) = self.rnn(x, hidden_states)\n",
    "        x = self.act(self.decoder(x_lstm))\n",
    "        x = self.decoder2(x) + self.out_bias\n",
    "        return x, (h1, c1)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "        CLOSURE FUNCTION\n",
    "    '''\n",
    "    def criterion(self, y_pred, y_true):\n",
    "        #return torch.sum(torch.square(y_pred - y_true))\n",
    "        return nn.MSELoss()(y_pred, y_true)\n",
    "        \n",
    "    \n",
    "    def Vocv_func(self, SOC):\n",
    "        Vocv_SOC = torch.tanh(self.Vocv_l1(SOC))\n",
    "        Vocv_SOC = torch.square(self.Vocv_l2(Vocv_SOC))\n",
    "        return self.Vocv_0 + Vocv_SOC\n",
    "    \n",
    "    def R0_func(self, SOC):\n",
    "        R0_SOC = torch.tanh(self.R0_l1(SOC))\n",
    "        R0_SOC = torch.square(self.R0_l2(R0_SOC))\n",
    "        return R0_SOC\n",
    "    \n",
    "    def R1_func(self, SOC):\n",
    "        R1_SOC = torch.tanh(self.R1_l1(SOC))\n",
    "        R1_SOC = torch.square(self.R1_l2(R1_SOC))\n",
    "        return R1_SOC\n",
    "    \n",
    "    def R2_func(self, SOC):\n",
    "        R2_SOC = torch.tanh(self.R2_l1(SOC))\n",
    "        R2_SOC = torch.square(self.R2_l2(R2_SOC))\n",
    "        return R2_SOC\n",
    "    \n",
    "    def C1_func(self, SOC):\n",
    "        C1_SOC = torch.tanh(self.C1_l1(SOC))\n",
    "        C1_SOC = torch.square(self.C1_l2(C1_SOC))\n",
    "        return C1_SOC\n",
    "    \n",
    "    def C2_func(self, SOC):\n",
    "        C2_SOC = torch.tanh(self.C2_l1(SOC))\n",
    "        C2_SOC = torch.square(self.C2_l2(C2_SOC))\n",
    "        return C2_SOC\n",
    "    \n",
    "    def compute_ECM(self, I, SOC_t0, dt_, I1_t0, I2_t0):\n",
    "        \n",
    "        # Initialise values\n",
    "        nt = I.size(1)\n",
    "        SOC = torch.zeros_like(I, requires_grad=True)\n",
    "        I1 = torch.zeros_like(I, requires_grad=True)\n",
    "        I2 = torch.zeros_like(I, requires_grad=True)\n",
    "        V0 = torch.zeros_like(I, requires_grad=True)\n",
    "        V1 = torch.zeros_like(I, requires_grad=True)\n",
    "        V2 = torch.zeros_like(I, requires_grad=True)\n",
    "        #Vocv = torch.zeros_like(I, requires_grad=True)\n",
    "        \n",
    "        # Make indexing tensor to allow specific values to change each timestep\n",
    "        idx = torch.zeros_like(I, requires_grad=False)\n",
    "        idx[:,0] = 1.0\n",
    "        idx.requires_grad=True\n",
    "        \n",
    "        # Change in SOC at each timestep\n",
    "        dSOC = torch.cumsum((I[:,:,:] * dt_[:,:,:]) / self.Q_nominal, dim=1)\n",
    "        dSOC = torch.concat((torch.zeros_like(dSOC[:,0:1,:]), dSOC[:,:-1,:]), dim=1)\n",
    "        \n",
    "        # Initial SOC\n",
    "        SOC = SOC + SOC_t0.reshape(-1, 1, 1).tile(1, nt, 1) + dSOC\n",
    "                \n",
    "        R0 = self.R0_func(SOC)\n",
    "        R1 = self.R1_func(SOC)\n",
    "        R2 = self.R2_func(SOC)\n",
    "        C1 = self.C1_func(SOC)\n",
    "        C2 = self.C2_func(SOC)\n",
    "        \n",
    "        # Calculate values at each timestep\n",
    "        for t in range(nt):\n",
    "            \n",
    "            # Make indexing tensor to allow specific values to change each timestep\n",
    "            idx = torch.zeros_like(I, requires_grad=False)\n",
    "            idx[:,t,:] = 1.0\n",
    "            idx.requires_grad=True\n",
    "            \n",
    "            if t > 0:\n",
    "                I1 = I1 + idx * ((torch.exp(-dt_[:,t,:]/(R1[:,t,:]*C1[:,t,:])) * I1[:,t-1,:]) + \n",
    "                                 ((torch.ones_like(I[:,0,:], requires_grad=True) - torch.exp(-dt_[:,t,:]/(R1[:,t,:]*C1[:,t,:]))) * I[:,t-1,:])\n",
    "                                ).unsqueeze(1)\n",
    "\n",
    "                I2 = I2 + idx * ((torch.exp(-dt_[:,t,:]/(R2[:,t,:]*C2[:,t,:])) * I2[:,t-1,:]) + \n",
    "                                 ((torch.ones_like(I[:,0,:], requires_grad=True) - torch.exp(-dt_[:,t,:]/(R2[:,t,:]*C2[:,t,:]))) * I[:,t-1,:])\n",
    "                                ).unsqueeze(1)\n",
    "            else:\n",
    "                I1 = I1 + idx * ((torch.exp(-dt_[:,t,:]/(R1[:,t,:]*C1[:,t,:])) * I1_t0.reshape(-1, 1)) + \n",
    "                                 ((torch.ones_like(I[:,0,:], requires_grad=True) - torch.exp(-dt_[:,t,:]/(R1[:,t,:]*C1[:,t,:]))) * I[:,t,:])\n",
    "                                ).unsqueeze(1)\n",
    "\n",
    "                I2 = I2 + idx * ((torch.exp(-dt_[:,t,:]/(R2[:,t,:]*C2[:,t,:])) * I2_t0.reshape(-1, 1)) + \n",
    "                                 ((torch.ones_like(I[:,0,:], requires_grad=True) - torch.exp(-dt_[:,t,:]/(R2[:,t,:]*C2[:,t,:]))) * I[:,t,:])\n",
    "                                ).unsqueeze(1)\n",
    "            \n",
    "        V0 = I * R0\n",
    "        V1 = I * R1\n",
    "        V2 = I * R2\n",
    "            \n",
    "        Vocv = self.Vocv_func(SOC)\n",
    "            \n",
    "        V = torch.relu(Vocv + V0 + V1 + V2)  # Total voltage across cell\n",
    "        \n",
    "        # Return data\n",
    "        return(SOC, V, I1, I2)\n",
    "    \n",
    "    \n",
    "    def closure(self, y_data, x, hidden_states, I1_t0, I2_t0):\n",
    "        \n",
    "        h1, c1 = hidden_states\n",
    "\n",
    "        # predict\n",
    "        y_pred, (h1, c1) = self.forward(x, (h1, c1))\n",
    "        \n",
    "        SOC_theory, V_theory, I1_theory, I2_theory = self.compute_ECM(x[:,:,0:1], x[:,0:1,1:2], x[:,:,2:3], I1_t0, I2_t0)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss_theory = self.criterion(y_pred, V_theory)\n",
    "        loss_data   = self.criterion(y_pred, y_data)\n",
    "        loss_ecm    = self.criterion(V_theory, y_data)\n",
    "        \n",
    "        loss = (self.L_data * loss_data) + (self.L_theory * loss_theory) + (self.L_ECM * loss_ecm)\n",
    "        \n",
    "        self.data_loss_batch.append(self.L_data.item() * loss_data.item())\n",
    "        self.theory_loss_batch.append(self.L_theory.item() * loss_theory.item())\n",
    "        self.ecm_loss_batch.append(self.L_ECM.item() * loss_ecm.item())\n",
    "        \n",
    "        return loss, (h1, c1), I1_theory[:,-1,:], I2_theory[:,-1,:]\n",
    "    \n",
    "    \n",
    "    def closure_ecm_only(self, y_data, x, hidden_states, I1_t0, I2_t0):\n",
    "        \n",
    "        SOC_theory, V_theory, I1_theory, I2_theory = self.compute_ECM(x[:,:,0:1], x[:,0:1,1:2], x[:,:,2:3], I1_t0, I2_t0)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss_ecm = self.criterion(V_theory, y_data) + self.criterion(torch.diff(V_theory, dim=1), torch.diff(y_data, dim=1)) * self.L_ECM\n",
    "\n",
    "        self.ecm_loss_batch.append(self.L_ECM.item() * loss_ecm.item())\n",
    "        \n",
    "        return loss_ecm, hidden_states, I1_theory[:,-1,:], I2_theory[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5ed734",
   "metadata": {
    "id": "3f5ed734"
   },
   "outputs": [],
   "source": [
    "model = ECM_PINN(2,25,25,1)\n",
    "model.Q_nominal = data_file[0]['Q_nominal']\n",
    "\n",
    "optim     = torch.optim.Adam(model.parameters(), lr=1.0e-3)\n",
    "ECM_optim = torch.optim.Adam(model.parameters(), lr=1.0e-3)\n",
    "\n",
    "ECM_path = \"D:/OneDrive - Queen's University Belfast/Documents/Thesis/Chapter2_ECM/Saved models/Nissan_data/trained_ECM.pth\"\n",
    "model.load_state_dict(torch.load(ECM_path)['model'].state_dict())\n",
    "\n",
    "loss_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd575c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7cfd575c",
    "outputId": "627a0ed3-5590-43d5-d17c-df597fab1d6c"
   },
   "outputs": [],
   "source": [
    "def plot_tests_standard(model, cycle_num, steps, ecm_only=False):\n",
    "    model.eval()\n",
    "    \n",
    "    n_tests = 1  # number of test batches\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        V_pred   = np.asarray([])\n",
    "        V_ecm    = np.asarray([])\n",
    "        SOC_ecm  = np.asarray([])\n",
    "        SOC_pred = np.asarray([])\n",
    "        I1_pred  = np.asarray([])\n",
    "        I2_pred  = np.asarray([])\n",
    "        \n",
    "        h0 = torch.zeros(model.n_lstm, n_tests, model.H3)\n",
    "        c0 = torch.zeros(model.n_lstm, n_tests, model.H3)\n",
    "        \n",
    "        I1 = torch.zeros((1,1,1))\n",
    "        I2 = torch.zeros((1,1,1))\n",
    "\n",
    "        for step in tqdm(range(steps)):\n",
    "\n",
    "            Time  = torch.tensor(data_file[cycle_num]['time'][step*step_size:(step+1)*step_size],  requires_grad=False, dtype=my_dtype).unsqueeze(0)\n",
    "            dt_  = Time[:,1:,:] - Time[:,:-1,:]\n",
    "            dt_  = torch.concat((dt_, dt_[:,-1:,:]), dim=1)\n",
    "            \n",
    "            Imeas = torch.tensor(data_file[cycle_num]['Imeas'][step*step_size:(step+1)*step_size], requires_grad=False, dtype=my_dtype).unsqueeze(0)\n",
    "            Vmeas = torch.tensor(data_file[cycle_num]['Vmeas'][step*step_size:(step+1)*step_size], requires_grad=False, dtype=my_dtype).unsqueeze(0)\n",
    "            SOC_batch = SOC[cycle_num,step*step_size:(step+1)*step_size,:].unsqueeze(0).type(my_dtype)\n",
    "\n",
    "            x_batch = torch.concat((Imeas, SOC_batch, dt_), dim=-1)\n",
    "            \n",
    "            if not ecm_only:\n",
    "                y_pred, (h1, c1) = model(x_batch, (h0, c0))\n",
    "                V_pred = np.append(V_pred, y_pred[0,:,0].cpu().numpy(), axis=0)\n",
    "\n",
    "                h0 = h1\n",
    "                c0 = c1\n",
    "                        \n",
    "            SOC_ecm_, V_ecm_, I1, I2 = model.compute_ECM(Imeas, SOC_batch[:,0:1,:], dt_, I1[:,-1:,:], I2[:,-1:,:])\n",
    "            \n",
    "            V_ecm  = np.append(V_ecm,  V_ecm_[0,:,0].cpu().numpy(), axis=0)\n",
    "            SOC_ecm  = np.append(SOC_ecm,  SOC_ecm_[0,:,0].cpu().numpy(), axis=0)\n",
    "            I1_pred = np.append(I1_pred, I1[0,:,0].cpu().numpy(), axis=0)\n",
    "            I2_pred = np.append(I2_pred, I2[0,:,0].cpu().numpy(), axis=0)\n",
    "\n",
    "            \n",
    "        if ecm_only:\n",
    "            # Plot voltage\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.plot(data_file[cycle_num]['time'][:len(V_ecm)].cpu(), data_file[cycle_num]['Vmeas'][:len(V_ecm)].cpu(), 'k--', alpha=0.8, label='Measured')\n",
    "            plt.plot(data_file[cycle_num]['time'][:len(V_ecm)].cpu(), V_ecm, 'b', label='ECM Prediction')\n",
    "            plt.ylabel('Voltage (V)')\n",
    "            plt.xlabel('Time (s)')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot currents through branches\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.plot(data_file[cycle_num]['time'][:len(V_ecm)].cpu(), data_file[cycle_num]['Imeas'][:len(V_ecm)].cpu(), 'k', alpha=0.8, label='Measured total I')\n",
    "            plt.plot(data_file[cycle_num]['time'][:len(V_ecm)].cpu(), I1_pred, label='I1')\n",
    "            plt.plot(data_file[cycle_num]['time'][:len(V_ecm)].cpu(), I2_pred, label='I2')\n",
    "            plt.ylabel('Voltage (V)')\n",
    "            plt.xlabel('Time (s)')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "        else:\n",
    "            # Plot voltage\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.plot(data_file[cycle_num]['time'][:len(V_pred)].cpu(), data_file[cycle_num]['Vmeas'][:len(V_pred)].cpu(), 'k--', alpha=0.8, label='Measured')\n",
    "            plt.plot(data_file[cycle_num]['time'][:len(V_pred)].cpu(), V_pred, 'r', label='Prediction')\n",
    "            plt.plot(data_file[cycle_num]['time'][:len(V_pred)].cpu(), V_ecm, 'b', label='ECM Prediction')\n",
    "            plt.ylabel('Voltage (V)')\n",
    "            plt.xlabel('Time (s)')\n",
    "            plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5691f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_samples = nData\n",
    "batch_size = len(data_file)\n",
    "batches = 1 #-(-n_samples // batch_size)\n",
    "\n",
    "step_size = 100\n",
    "steps = N // step_size #-(-N // step_size)    # Round up to nearest integer (so end point that don't make a complete batch are still included)\n",
    "\n",
    "increase_steps = False\n",
    "step_gain = 10\n",
    "\n",
    "random_steps = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a310c754",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a310c754",
    "outputId": "5377e257-47c5-4db9-ab96-c3d412d96afc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Record learning process\n",
    "model.L_data   = torch.tensor(0.0, requires_grad=True)\n",
    "model.L_theory = torch.tensor(0.0, requires_grad=True)\n",
    "model.L_ECM    = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "Epochs = 10000 #- len(loss_train)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for ep in tqdm(range(Epochs)):\n",
    "    \n",
    "    model.epochs += 1\n",
    "    loss_batch = []\n",
    "    model.data_loss_batch = []\n",
    "    model.theory_loss_batch = []\n",
    "    model.ecm_loss_batch = []\n",
    "    \n",
    "    # Shuffle input and output values\n",
    "    idx = torch.randperm(len(data_file))\n",
    "    \n",
    "    if increase_steps:\n",
    "        if step_gain > steps:\n",
    "            steps_this_ep = steps\n",
    "        else:\n",
    "            if model.epochs > 1:\n",
    "                if loss_train[-1] < 1.0:\n",
    "                    step_gain += 1\n",
    "                    print(f'Training steps increased to {step_gain}')\n",
    "            steps_this_ep = step_gain\n",
    "\n",
    "    else:\n",
    "        steps_this_ep = steps\n",
    "    \n",
    "    if random_steps:\n",
    "        steps_this_ep = int(steps_this_ep * torch.rand((1,)).item())\n",
    "        if steps_this_ep < 1:\n",
    "            steps_this_ep = 1\n",
    "    else:\n",
    "        steps_this_ep = steps_this_ep\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "    for batch in range(batches):\n",
    "        \n",
    "        batch_start = batch * batch_size\n",
    "        batch_end = (batch+1) * batch_size\n",
    "\n",
    "        h1 = torch.zeros(model.n_lstm, batch_end - batch_start, model.H3)\n",
    "        c1 = torch.zeros(model.n_lstm, batch_end - batch_start, model.H3)\n",
    "        \n",
    "        I1_t0 = torch.zeros((batch_size, 1), requires_grad=True)\n",
    "        I2_t0 = torch.zeros((batch_size, 1), requires_grad=True)\n",
    "                \n",
    "        for step in range(steps_this_ep):\n",
    "\n",
    "            hidden_states = (h1, c1)\n",
    "            \n",
    "            Time      = torch.zeros((batch_size, step_size, 1))\n",
    "            Imeas     = torch.zeros_like(Time)\n",
    "            Vmeas     = torch.zeros_like(Time)\n",
    "            SOC_batch = torch.zeros_like(Time)\n",
    "\n",
    "            if (step+1)*step_size < len(data_file[0]['time']):\n",
    "                \n",
    "                for ii in range(batch_size):                \n",
    "                    Time[ii]      = data_file[ii]['time'][step*step_size:(step+1)*step_size].unsqueeze(0)\n",
    "                    Imeas[ii]     = data_file[ii]['Imeas'][step*step_size:(step+1)*step_size].unsqueeze(0)\n",
    "                    Vmeas[ii]     = data_file[ii]['Vmeas'][step*step_size:(step+1)*step_size].unsqueeze(0)\n",
    "                    SOC_batch[ii] = SOC[ii, step*step_size:(step+1)*step_size, :].type(my_dtype)\n",
    "                    \n",
    "            else:\n",
    "                for ii in range(batch_size):    \n",
    "                    Time[ii]      = data_file[ii]['time'][step*step_size:].unsqueeze(0)\n",
    "                    Imeas[ii]     = data_file[ii]['Imeas'][step*step_size:].unsqueeze(0)\n",
    "                    Vmeas[ii]     = data_file[ii]['Vmeas'][step*step_size:].unsqueeze(0)\n",
    "                    SOC_batch[ii] = SOC[ii, step*step_size:, :].type(my_dtype)\n",
    "                \n",
    "            dt_  = Time[:,1:,:] - Time[:,:-1,:]\n",
    "            dt_  = torch.concat((dt_, dt_[:,-1:,:]), dim=1)\n",
    "                \n",
    "            x_batch = torch.concat((Imeas.detach(), SOC_batch.detach(), dt_.detach()), dim=-1)\n",
    "            y_batch = Vmeas.detach()\n",
    "\n",
    "            x_batch.requires_grad = True\n",
    "            y_batch.requires_grad = True\n",
    "\n",
    "            model.zero_grad()\n",
    "            optim.zero_grad()\n",
    "\n",
    "            loss_, hidden_states, I1_t0, I2_t0 = model.closure(y_batch, x_batch, hidden_states, I1_t0, I2_t0)\n",
    "            #loss_ecm, hidden_states, I1_t0, I2_t0 = model.closure_ecm_only(y_batch, x_batch, hidden_states, I1_t0, I2_t0)\n",
    "            \n",
    "            loss_.backward()\n",
    "            optim.step()\n",
    "\n",
    "            loss_batch = np.append(loss_batch, loss_.clone().item())\n",
    "\n",
    "            h1, c1 = hidden_states\n",
    "            h1 = h1.clone().detach()\n",
    "            c1 = c1.clone().detach()\n",
    "            I1_t0 = I1_t0.clone().detach()\n",
    "            I2_t0 = I2_t0.clone().detach()\n",
    "\n",
    "            h1.requires_grad = True\n",
    "            c1.requires_grad = True\n",
    "            I1_t0.requires_grad = True\n",
    "            I2_t0.requires_grad = True\n",
    "            \n",
    "        #if batch % 1 == 0:\n",
    "            #print(f\"Batch {batch+1}/{batches}\\t|  Loss: {np.mean(loss_batch)}\")\n",
    "            \n",
    "    loss_train.append(np.mean(loss_batch))\n",
    "    model.data_loss.append(np.mean(model.data_loss_batch))\n",
    "    model.theory_loss.append(np.mean(model.theory_loss_batch))\n",
    "    model.ecm_loss.append(np.mean(model.ecm_loss_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd198a48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fd198a48",
    "outputId": "4114553b-ce43-423c-a330-d7f9a86b6848"
   },
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(loss_train, 'tab:blue', label='Total')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(model.data_loss,  'tab:red',  label='Data', alpha=1.0)\n",
    "plt.plot(model.theory_loss,  'tab:blue',  label='Theory', alpha=1.0)\n",
    "plt.plot(model.ecm_loss,  'tab:orange',  label='ECM params', alpha=1.0)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.yscale('log')\n",
    "plt.xlim([0, len(model.data_loss)])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(model.ecm_loss,  'tab:orange',  label='ECM params', alpha=1.0)\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.yscale('log')\n",
    "plt.xlim([0, len(model.data_loss)])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d4c78f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for cycle_num in range(len(data_file)):\n",
    "    print(file_path[cycle_num]['name'])\n",
    "    plot_tests_standard(model, cycle_num, steps, ecm_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc902dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    v = model.Vocv_func(torch.linspace(0,1,50).reshape(-1,1))\n",
    "    \n",
    "    plt.plot(torch.linspace(0,1,50).cpu(), v[:,0].cpu())\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    v = model.R0_func(torch.linspace(0,1,50).reshape(-1,1))\n",
    "    \n",
    "    plt.plot(torch.linspace(0,1,50).cpu(), v[:,0].cpu())\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    v = model.R1_func(torch.linspace(0,1,50).reshape(-1,1))\n",
    "    \n",
    "    plt.plot(torch.linspace(0,1,50).cpu(), v[:,0].cpu())\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    v = model.R2_func(torch.linspace(0,1,50).reshape(-1,1))\n",
    "    \n",
    "    plt.plot(torch.linspace(0,1,50).cpu(), v[:,0].cpu())\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5504d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'ECM_PINN_v4-6_Corrections.pth'\n",
    "\n",
    "torch.save({\n",
    "            'model': model,\n",
    "            'optimizer_state_dict': optim.state_dict(),\n",
    "            'loss': loss_train,\n",
    "            }, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed4dd06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
